# Google Drive Setup Guide

This guide explains what files should be uploaded to Google Drive (files that are too large or shouldn't be in GitHub).

## Files to Upload to Google Drive

### 1. Model Checkpoints (Required - Large Files)

These are the trained model weights. **Total size: ~500MB - 2GB**

```
✅ Upload these .pth files:
├── best_resnet50_base.pth              (~300MB)
├── final_best_tuned_model.pth         (~300MB)  [ResNet50 tuned]
├── best_efficientnet_b0.pth            (~60MB)
├── best_efficientnet_b0_tuned.pth      (~60MB)
├── best_vgg16_bn.pth                   (~500MB)
├── best_vgg16_bn_tuned.pth             (~500MB)
├── best_vit_b_16_base.pth              (~1.1GB)
└── best_vit_b_16_tuned.pth             (~1.1GB)
```

**Why:** These files are large (hundreds of MB to GB each) and are already excluded from GitHub via `.gitignore`. They're essential for:
- Reproducing results
- Running evaluations without retraining
- Loading models for inference

### 2. Dataset Images (Required - Very Large)

The entire `artset/` folder containing all training images.

```
✅ Upload:
└── artset/
    ├── Abstract_Expressionism/
    ├── Art_Nouveau_Modern/
    ├── Baroque/
    ├── ... (all 20 style folders)
    └── Ukiyo_e/
```

**Why:** 
- Contains ~18,000 images (likely several GB)
- Already excluded from GitHub (images in `.gitignore`)
- Required to run training/evaluation scripts
- Too large for GitHub

**Note:** If you have the original dataset source, you could also just keep a download script instead.

### 3. Confusion Matrix Images (Optional - Medium Size)

The generated confusion matrix PNG files.

```
✅ Optional - Upload:
└── reports/
    └── confusion_matrices/
        ├── resnet50/
        │   ├── test_confusion_matrix_normalized.png
        │   └── test_confusion_matrix_raw.png
        ├── efficientnet_b0/
        ├── vgg16_bn/
        └── vit_b_16/
```

**Why:** 
- Already excluded from GitHub (PNG files in `.gitignore`)
- Useful for documentation/presentations
- Can be regenerated, so optional
- Total size: ~5-20MB

### 4. Training History JSON Files (Optional - Small)

These are small but contain detailed training metrics.

```
✅ Optional - Upload:
├── training_history_resnet.json
├── training_history_efficientnet.json
├── training_history_vgg16.json
├── training_history_vit.json
└── training_history.json
```

**Why:**
- Small files (<1MB each)
- Useful for analysis and plotting
- Can be regenerated by retraining
- **Note:** These are NOT in `.gitignore`, so they might be in GitHub already

### 5. Reports Directory (Optional)

Complete reports folder with all evaluation results.

```
✅ Optional - Upload entire:
└── reports/
    ├── confusion_matrices/     (PNG files)
    ├── ensemble_results.json
    ├── ensemble_summary.txt
    └── ... (all other reports)
```

**Why:**
- Contains all evaluation results
- Useful for documentation
- Can be regenerated, so optional

## Recommended Google Drive Structure

Organize your Google Drive folder like this:

```
dlcv_data/
├── checkpoints/
│   ├── resnet50/
│   │   ├── best_resnet50_base.pth
│   │   └── final_best_tuned_model.pth
│   ├── efficientnet_b0/
│   │   ├── best_efficientnet_b0.pth
│   │   └── best_efficientnet_b0_tuned.pth
│   ├── vgg16_bn/
│   │   ├── best_vgg16_bn.pth
│   │   └── best_vgg16_bn_tuned.pth
│   └── vit_b_16/
│       ├── best_vit_b_16_base.pth
│       └── best_vit_b_16_tuned.pth
│
├── dataset/
│   └── artset/              (entire folder with all images)
│
└── reports/                  (optional)
    ├── confusion_matrices/
    ├── ensemble_results.json
    └── ...
```

## What NOT to Upload (Keep in GitHub)

These files should stay in GitHub:

- ✅ All Python scripts (`.py` files)
- ✅ Configuration files (`config.py`, `requirements.txt`)
- ✅ README and documentation
- ✅ Small JSON metadata files (`data/metadata/*.json`)
- ✅ Code structure and utilities

## Download Instructions for Others

If someone needs to use your Google Drive files, provide these instructions:

1. **Download checkpoints:**
   ```
   Download all .pth files from Google Drive
   Place them in the project root directory
   ```

2. **Download dataset:**
   ```
   Download the artset/ folder
   Place it in the project root directory (./artset/)
   ```

3. **Verify setup:**
   ```bash
   # Check that checkpoints exist
   ls *.pth
   
   # Check that dataset exists
   ls artset/
   ```

## Quick Upload Checklist

- [ ] Upload all `.pth` checkpoint files (8 files)
- [ ] Upload `artset/` folder with all images
- [ ] (Optional) Upload `reports/confusion_matrices/` PNG files
- [ ] (Optional) Upload training history JSON files
- [ ] Create a README in Google Drive explaining the structure

## File Size Estimates

| Category | Estimated Size |
|----------|----------------|
| Model Checkpoints | ~2-3 GB |
| Dataset (artset/) | ~5-15 GB (depends on image resolution) |
| Confusion Matrices | ~5-20 MB |
| Training History | ~1-5 MB |
| **Total** | **~7-18 GB** |

## Alternative: Use Google Drive File Stream

If you have Google Drive File Stream installed, you can:
1. Create a folder in Google Drive
2. Use it as a mounted drive
3. Create symlinks or copy files as needed

This makes it easier to access files without downloading everything.

